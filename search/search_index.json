{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Author: Emma LeRoy Advisor: Tyson Lee Swetnam About \u00b6 Emma LeRoy is a rising senior at University High School in Tucson, Arizona. During the summer of 2022 she is an intern at The University of Arizona through the BIO5 Institute's KEYS program. This summer she is an intern working at The University of Arizona in Dr. Tyson L. Swetnam's lab: a member of CyVerse , a cutting edge cyberinfrastructure funded by the National Science Foundation that is designed for research and committed to the principles of open science. This website follows the FAIR and CARE data principles and hopes to help further open science.","title":"Introduction"},{"location":"#introduction","text":"Author: Emma LeRoy Advisor: Tyson Lee Swetnam","title":"Introduction"},{"location":"#about","text":"Emma LeRoy is a rising senior at University High School in Tucson, Arizona. During the summer of 2022 she is an intern at The University of Arizona through the BIO5 Institute's KEYS program. This summer she is an intern working at The University of Arizona in Dr. Tyson L. Swetnam's lab: a member of CyVerse , a cutting edge cyberinfrastructure funded by the National Science Foundation that is designed for research and committed to the principles of open science. This website follows the FAIR and CARE data principles and hopes to help further open science.","title":"About"},{"location":"STACcatalogs/","text":"STAC Catalogs \u00b6 What Are They? \u00b6 Oftentimes raw data is not ready for analysis and requires pre-processing and the installation of a significant number of software packages allows for creation of metadata to make datasets indexable and discoverable promotes reproducibility, open data, and open science STAC specification means that these large datasets can be used on the cloud as opposed to being downloaded locally, shortens run time and improves ease of use Figure credit : STAC Specification official logo from the STAC Index Site Structure \u00b6 composed of spatio-temporal assets- a piece of planetary data collected at a specific time with a specific location STAC specification designates inclusion of metadata on time and location of data, thumbnail for searching and discovery, applicable links to raw data, key words to describe relationship of data and point to similar spatio-temporal assets Users can further customize this metadata Spatio-temporal assets are a type of Javascript Object Notation (JSON) File JSON files are a data interchange format that allow datasets to be displayed in a text based format How Are They Used? \u00b6 used to index data, making it searchable and discoverable data standard for formatting and creating metadata for geospatial data promotes open data and reproducibility allows data to be shared, understood, and discovered used for geospatial data about the Earth; however can also be used for planetary data Can be viewed as a webpage using a STAC Browser Use in Google's Earth Engine \u00b6 The GEE Public Data Catalog can be viewed as a STAC catalog , STAC browser , or HTML version allows searchability, discovery, and improves user functionality","title":"STAC Catalogs"},{"location":"STACcatalogs/#stac-catalogs","text":"","title":"STAC Catalogs"},{"location":"STACcatalogs/#what-are-they","text":"Oftentimes raw data is not ready for analysis and requires pre-processing and the installation of a significant number of software packages allows for creation of metadata to make datasets indexable and discoverable promotes reproducibility, open data, and open science STAC specification means that these large datasets can be used on the cloud as opposed to being downloaded locally, shortens run time and improves ease of use Figure credit : STAC Specification official logo from the STAC Index Site","title":"What Are They?"},{"location":"STACcatalogs/#structure","text":"composed of spatio-temporal assets- a piece of planetary data collected at a specific time with a specific location STAC specification designates inclusion of metadata on time and location of data, thumbnail for searching and discovery, applicable links to raw data, key words to describe relationship of data and point to similar spatio-temporal assets Users can further customize this metadata Spatio-temporal assets are a type of Javascript Object Notation (JSON) File JSON files are a data interchange format that allow datasets to be displayed in a text based format","title":"Structure"},{"location":"STACcatalogs/#how-are-they-used","text":"used to index data, making it searchable and discoverable data standard for formatting and creating metadata for geospatial data promotes open data and reproducibility allows data to be shared, understood, and discovered used for geospatial data about the Earth; however can also be used for planetary data Can be viewed as a webpage using a STAC Browser","title":"How Are They Used?"},{"location":"STACcatalogs/#use-in-googles-earth-engine","text":"The GEE Public Data Catalog can be viewed as a STAC catalog , STAC browser , or HTML version allows searchability, discovery, and improves user functionality","title":"Use in Google's Earth Engine"},{"location":"communityDataset/","text":"Awesome-gee-community-datasets \u00b6 Flowchart for the STAC specification used with the community datasets \u00b6 graph LR B[Community Datasets JSON] --> C{title, sample code, type, id, provider, tags}; A[Community Datasets CSV] --> C{title, sample code, type, id, provider, tags}; C --> D[additional JSONs to describe corresponding datasets in the catalog]; Flowchart for the project markdown files that are rendering on the github.io site \u00b6 graph LR A[Overarching Category] --> B{Markdown file for the page}; B --->|Render on github.io site| C[Description, Citation, Earth Engine Snippet, DOI, License]; However, this information varies slightly for each dataset: - some are missing key info such as the DOI - others include different additional specific information on the dataset that are an exception from the norm - additionally, the descriptions vary heavily in detail between the datasets It appears that the user is contributing this information for the project file when they submit their dataset to be included in the project. There are procedures for submitting datasets, including what info to include; however, it seems that this isn't being strictly adhered to. Question: are the contributors also providing the information for the JSON files? because these seem much more correctly standardized to the STAC specification guidelines","title":"Awesome Community Datasets"},{"location":"communityDataset/#awesome-gee-community-datasets","text":"","title":"Awesome-gee-community-datasets"},{"location":"communityDataset/#flowchart-for-the-stac-specification-used-with-the-community-datasets","text":"graph LR B[Community Datasets JSON] --> C{title, sample code, type, id, provider, tags}; A[Community Datasets CSV] --> C{title, sample code, type, id, provider, tags}; C --> D[additional JSONs to describe corresponding datasets in the catalog];","title":"Flowchart for the STAC specification used with the community datasets"},{"location":"communityDataset/#flowchart-for-the-project-markdown-files-that-are-rendering-on-the-githubio-site","text":"graph LR A[Overarching Category] --> B{Markdown file for the page}; B --->|Render on github.io site| C[Description, Citation, Earth Engine Snippet, DOI, License]; However, this information varies slightly for each dataset: - some are missing key info such as the DOI - others include different additional specific information on the dataset that are an exception from the norm - additionally, the descriptions vary heavily in detail between the datasets It appears that the user is contributing this information for the project file when they submit their dataset to be included in the project. There are procedures for submitting datasets, including what info to include; however, it seems that this isn't being strictly adhered to. Question: are the contributors also providing the information for the JSON files? because these seem much more correctly standardized to the STAC specification guidelines","title":"Flowchart for the project markdown files that are rendering on the github.io site"},{"location":"cyverse/","text":"CyVerse \u00b6 Introduction \u00b6 CyVerse serves as a way for life scientists to share their data with others around the world through a common cyberinfrastructure. It is funded by the National Science Foudation's Directorate for Biological Science and is currently led by the University of Arizona. CyVerse's cyberinfrastructure allows scientists to store their data as well as share it with others through cloud computing for further analysis. Acting as a way to complete complex analyses and share large datasets, CyVerse furthers open science and enables a collaborative workspace. VICE Apps \u00b6 The Visual and Interactive Computing Environment(VICE) allows scientists to run interactive applications through CyVerse. Through this scientists can open their interactive applications(Jupyter Lab, RStudio, Shiny, WebGL, HTML5, VNC, and XPRA), transfer data into containters, analyze this data, and send their results to the cloud.","title":"CyVerse"},{"location":"cyverse/#cyverse","text":"","title":"CyVerse"},{"location":"cyverse/#introduction","text":"CyVerse serves as a way for life scientists to share their data with others around the world through a common cyberinfrastructure. It is funded by the National Science Foudation's Directorate for Biological Science and is currently led by the University of Arizona. CyVerse's cyberinfrastructure allows scientists to store their data as well as share it with others through cloud computing for further analysis. Acting as a way to complete complex analyses and share large datasets, CyVerse furthers open science and enables a collaborative workspace.","title":"Introduction"},{"location":"cyverse/#vice-apps","text":"The Visual and Interactive Computing Environment(VICE) allows scientists to run interactive applications through CyVerse. Through this scientists can open their interactive applications(Jupyter Lab, RStudio, Shiny, WebGL, HTML5, VNC, and XPRA), transfer data into containters, analyze this data, and send their results to the cloud.","title":"VICE Apps"},{"location":"githubactions/","text":"Introduction \u00b6 GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts. Key Vocabulary \u00b6 Workflows \u00b6 Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event. Events \u00b6 Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests. Runners \u00b6 Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub. Jobs \u00b6 A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order. Steps \u00b6 A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data. Actions \u00b6 Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs","title":"Githubactions"},{"location":"githubactions/#introduction","text":"GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts.","title":"Introduction"},{"location":"githubactions/#key-vocabulary","text":"","title":"Key Vocabulary"},{"location":"githubactions/#workflows","text":"Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event.","title":"Workflows"},{"location":"githubactions/#events","text":"Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests.","title":"Events"},{"location":"githubactions/#runners","text":"Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub.","title":"Runners"},{"location":"githubactions/#jobs","text":"A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order.","title":"Jobs"},{"location":"githubactions/#steps","text":"A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data.","title":"Steps"},{"location":"githubactions/#actions","text":"Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs","title":"Actions"},{"location":"githubed/","text":"GitHub Education \u00b6 Steps to Enroll \u00b6 Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack! What's Included and Functionality \u00b6 As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"GitHub Education Access"},{"location":"githubed/#github-education","text":"","title":"GitHub Education"},{"location":"githubed/#steps-to-enroll","text":"Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack!","title":"Steps to Enroll"},{"location":"githubed/#whats-included-and-functionality","text":"As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"What's Included and Functionality"},{"location":"googleEarthEngine/","text":"Google's Earth Engine \u00b6 Use \u00b6 Established in 2010 as a geospatial analysis platform run in the cloud (allows analysis of large datasets) public datasets promotes open science and reproducibility cloud computing allows planetary scale analysis users can upload their own datasets and write their own scripts to perform analyses The development of GEE as a free resource creates equity within science brings a high level analysis tool to a wider range of people, which furthers open science and promotes inquiry, discovery, and collaboration Figure credit : Image created by Diana Krupnik for article on teaching using the GEE API APIs \u00b6 Javascript Python Rest Structure \u00b6 initially featured a data repository from the past 40 years of global remote sensing data now expanded to include vector, climate, demographic, and elevation data These datasets can be layered to perform complex geospatial analyses community datasets are open source Code Editor \u00b6 Web based IDE for writing scripts complex geospatial work flows made easy interactive environment for developing earth engine applications raster data type is a matrix of cells in grid format -- digital aerial photographs, scanned maps","title":"Google Earth Engine"},{"location":"googleEarthEngine/#googles-earth-engine","text":"","title":"Google's Earth Engine"},{"location":"googleEarthEngine/#use","text":"Established in 2010 as a geospatial analysis platform run in the cloud (allows analysis of large datasets) public datasets promotes open science and reproducibility cloud computing allows planetary scale analysis users can upload their own datasets and write their own scripts to perform analyses The development of GEE as a free resource creates equity within science brings a high level analysis tool to a wider range of people, which furthers open science and promotes inquiry, discovery, and collaboration Figure credit : Image created by Diana Krupnik for article on teaching using the GEE API","title":"Use"},{"location":"googleEarthEngine/#apis","text":"Javascript Python Rest","title":"APIs"},{"location":"googleEarthEngine/#structure","text":"initially featured a data repository from the past 40 years of global remote sensing data now expanded to include vector, climate, demographic, and elevation data These datasets can be layered to perform complex geospatial analyses community datasets are open source","title":"Structure"},{"location":"googleEarthEngine/#code-editor","text":"Web based IDE for writing scripts complex geospatial work flows made easy interactive environment for developing earth engine applications raster data type is a matrix of cells in grid format -- digital aerial photographs, scanned maps","title":"Code Editor"},{"location":"jupyter/","text":"","title":"Jupyter Notebooks"},{"location":"keysassignments/","text":"Keys Assignments \u00b6 Assignment 1: Internship Description \u00b6 This summer I am working in Dr. Tyson L. Swetnam's Lab. He is a Research Assistant Professor of Geoinformatics with a joint appointment in the School of Natural Resources and the Environment. Dr. Swetnam is part of the CyVerse initiative: a National Science Foundation funded cyberinfrastructure that promotes open source data, science, and collaboration in the biosciences. Dr. Swetnam's research is focused on using cyberinfrastructure to support and encourage reproducible research, along with geospatial analysis. The Lab's current projects include Open Dendro , an initiative to restructure outdated dendrochronological software written in C, R, and Fortran into Python. Dr. Swetnam is involved in a plethora of other projects, but all of his projects show a commitment to open science through the use of open data and open software to promote reproducible research in information science. The primary project that I will be focused on this summer is the improvement of spatio-temporal asset catalog (STAC) catalogs for use in Google's Earth Engine. Each spatio-temporal asset is a file or dataset that contains information about the Earth captured at a specific time. By streamlining this process and working on making these datasets more accessible it ensures equal access and promotes open science. Project Description \u00b6 This summer I will be working on improving the STAC catalogs for the awesome-gee-community-datasets. This is a project spearheaded by Dr. Swetnam's former graduate student and colleague, Samapriya Roy. The project is focused on improving datasets for community use with Google's Earth Engine Catalog, a tool for mapping satellite and geospatial data across the Earth's surface to detect changes and trends. The awesome-gee-community-datasets are a set of community gathered and organized geospatial data that is preprocessed to allow for easy use with Google's Earth Engine. This summer I will work on improving the spatial-temporal asset (STAC) catalogs for these datasets to improve ease of use and access. A STAC catalog serves as a standardized way of storing and indexing geospatial data for easy discovery and use. This creates a common language and format that ensures previous code doesn't need to be rewritten, promoting open data, software, and science. Assignment 2: Introduction to your Research \u00b6 Purpose: \u00b6 The purpose of my research is to standardize and process geospatial data for use with Google\u2019s Earth Engine for planetary-scale geospatial analysis. This improvement and standardization of geospatial data will be conducted using spatio-temporal asset catalogs (STAC), which will help make these complex datasets easier to index and discover in order to perform analyses of Earth\u2019s systems and conditions. Previous Research: \u00b6 Oftentimes raw data is not ready for analysis, and it requires pre-processing. Additionally, this analysis often requires the installation and utilization of many different software packages. This is especially true for large datasets that are being used for planetary analysis on an Earth and solar system scale. However, the use of the spatio-temporal asset catalog (STAC) specification allows for the creation of metadata that allows datasets to be indexable and discoverable. A spatio-temporal asset is a type of Javascript object notation (JSON) file. This is a data interchange format that allows large raw datasets to be text based. The STAC specification designates the need for metadata on the time and location of the data, a thumbnail, links to the raw data, and key words that describe the relationship of the data, pointing to similar spatio-temporal assets. Following this specification furthers cloud based computing and means that these large geospatial datasets don\u2019t need to be downloaded locally for planetary scale analysis. It is for these reasons that previous research has shown that the STAC specification is an important and highly useful method for designating metadata, especially for large planetary scale analysis (Fergason et al, 2018). Additionally, this STAC specification can be used for Google\u2019s Earth Engine (GEE), established in 2010. GEE initially featured a data repository of global satellite data from the past 40 years; however, this has expanded to now include vector, climate, demographic, and elevation data that can be layered to perform complex global geospatial analyses. Users can also upload their own datasets and write scripts to analyze this data. The development of GEE as a free resource has helped to level the playing field and make large-scale geospatial analysis possible for a larger range of scientists, particularly those in developing countries (Kumar and Mutanga, 2018). However, a lot of this data that users are looking to use with GEE is not preproccessed, and STAC catalogs serve as a potential solution to this. My project will allow more equitable acces to a variety of geospatial data, promoting equality and open science. By improving the use of STAC catalogs it ensures that scientists have access to quality analysis tools like GEE and datasets that are easy to navigate. Need For Study: \u00b6 This study will further the use of STAC catalogs for geospatial data. Through this standardization, it will make Google\u2019s Earth Engine more accessible to other researchers and data scientists. Pre-processing geospatial data and formatting it using STAC catalogs makes this data easier to index and search, allowing for its discovery. This concept of open data is crucial in regard to the open science movement. The production of open software and data allows key geospatial analysis tools to reach a larger number of researchers and scientists. With the threat of climate change and the necessity for immediate action, it is crucial to understand Earth\u2019s systems and processes. Google\u2019s Earth Engine allows for large-scale geospatial analysis that can help scientists visualize and understand concepts such as forest fire management and sea level change. The improvement of STAC catalogs for Google\u2019s Earth Engine facilitates analysis and discovery that is crucial to understanding how to prevent climate change and preserve the natural environment. Problem Statement: \u00b6 Many current geospatial datasets are difficult to navigate, discover, and analyze. How can the organization of this data be improved to ensure ease of use, appropriate analysis, and equitable access? References \u00b6 Fergason, R. L., Hunter, M. A., Laura, J. R., Hare, T. M., & U.S. Geological Survey. (2021). Analysis Ready Data Available Through the SpatioTemporal Asset Catalog (STAC) Specification: Investigating the Application to Planetary Data. 5 th Planetary Data and PSIDA 2021, 2549, 7023\u20137024. https://www.hou.usra.edu/meetings/planetdata2021/pdf/7023.pdf Kumar, L., & Mutanga, O. (2018). Google Earth Engine Applications Since Inception: Usage, Trends, and Potential. Remote Sensing, 10(10), 1509. https://doi.org/10.3390/rs10101509","title":"Assignments"},{"location":"keysassignments/#keys-assignments","text":"","title":"Keys Assignments"},{"location":"keysassignments/#assignment-1-internship-description","text":"This summer I am working in Dr. Tyson L. Swetnam's Lab. He is a Research Assistant Professor of Geoinformatics with a joint appointment in the School of Natural Resources and the Environment. Dr. Swetnam is part of the CyVerse initiative: a National Science Foundation funded cyberinfrastructure that promotes open source data, science, and collaboration in the biosciences. Dr. Swetnam's research is focused on using cyberinfrastructure to support and encourage reproducible research, along with geospatial analysis. The Lab's current projects include Open Dendro , an initiative to restructure outdated dendrochronological software written in C, R, and Fortran into Python. Dr. Swetnam is involved in a plethora of other projects, but all of his projects show a commitment to open science through the use of open data and open software to promote reproducible research in information science. The primary project that I will be focused on this summer is the improvement of spatio-temporal asset catalog (STAC) catalogs for use in Google's Earth Engine. Each spatio-temporal asset is a file or dataset that contains information about the Earth captured at a specific time. By streamlining this process and working on making these datasets more accessible it ensures equal access and promotes open science.","title":"Assignment 1: Internship Description"},{"location":"keysassignments/#project-description","text":"This summer I will be working on improving the STAC catalogs for the awesome-gee-community-datasets. This is a project spearheaded by Dr. Swetnam's former graduate student and colleague, Samapriya Roy. The project is focused on improving datasets for community use with Google's Earth Engine Catalog, a tool for mapping satellite and geospatial data across the Earth's surface to detect changes and trends. The awesome-gee-community-datasets are a set of community gathered and organized geospatial data that is preprocessed to allow for easy use with Google's Earth Engine. This summer I will work on improving the spatial-temporal asset (STAC) catalogs for these datasets to improve ease of use and access. A STAC catalog serves as a standardized way of storing and indexing geospatial data for easy discovery and use. This creates a common language and format that ensures previous code doesn't need to be rewritten, promoting open data, software, and science.","title":"Project Description"},{"location":"keysassignments/#assignment-2-introduction-to-your-research","text":"","title":"Assignment 2: Introduction to your Research"},{"location":"keysassignments/#purpose","text":"The purpose of my research is to standardize and process geospatial data for use with Google\u2019s Earth Engine for planetary-scale geospatial analysis. This improvement and standardization of geospatial data will be conducted using spatio-temporal asset catalogs (STAC), which will help make these complex datasets easier to index and discover in order to perform analyses of Earth\u2019s systems and conditions.","title":"Purpose:"},{"location":"keysassignments/#previous-research","text":"Oftentimes raw data is not ready for analysis, and it requires pre-processing. Additionally, this analysis often requires the installation and utilization of many different software packages. This is especially true for large datasets that are being used for planetary analysis on an Earth and solar system scale. However, the use of the spatio-temporal asset catalog (STAC) specification allows for the creation of metadata that allows datasets to be indexable and discoverable. A spatio-temporal asset is a type of Javascript object notation (JSON) file. This is a data interchange format that allows large raw datasets to be text based. The STAC specification designates the need for metadata on the time and location of the data, a thumbnail, links to the raw data, and key words that describe the relationship of the data, pointing to similar spatio-temporal assets. Following this specification furthers cloud based computing and means that these large geospatial datasets don\u2019t need to be downloaded locally for planetary scale analysis. It is for these reasons that previous research has shown that the STAC specification is an important and highly useful method for designating metadata, especially for large planetary scale analysis (Fergason et al, 2018). Additionally, this STAC specification can be used for Google\u2019s Earth Engine (GEE), established in 2010. GEE initially featured a data repository of global satellite data from the past 40 years; however, this has expanded to now include vector, climate, demographic, and elevation data that can be layered to perform complex global geospatial analyses. Users can also upload their own datasets and write scripts to analyze this data. The development of GEE as a free resource has helped to level the playing field and make large-scale geospatial analysis possible for a larger range of scientists, particularly those in developing countries (Kumar and Mutanga, 2018). However, a lot of this data that users are looking to use with GEE is not preproccessed, and STAC catalogs serve as a potential solution to this. My project will allow more equitable acces to a variety of geospatial data, promoting equality and open science. By improving the use of STAC catalogs it ensures that scientists have access to quality analysis tools like GEE and datasets that are easy to navigate.","title":"Previous Research:"},{"location":"keysassignments/#need-for-study","text":"This study will further the use of STAC catalogs for geospatial data. Through this standardization, it will make Google\u2019s Earth Engine more accessible to other researchers and data scientists. Pre-processing geospatial data and formatting it using STAC catalogs makes this data easier to index and search, allowing for its discovery. This concept of open data is crucial in regard to the open science movement. The production of open software and data allows key geospatial analysis tools to reach a larger number of researchers and scientists. With the threat of climate change and the necessity for immediate action, it is crucial to understand Earth\u2019s systems and processes. Google\u2019s Earth Engine allows for large-scale geospatial analysis that can help scientists visualize and understand concepts such as forest fire management and sea level change. The improvement of STAC catalogs for Google\u2019s Earth Engine facilitates analysis and discovery that is crucial to understanding how to prevent climate change and preserve the natural environment.","title":"Need For Study:"},{"location":"keysassignments/#problem-statement","text":"Many current geospatial datasets are difficult to navigate, discover, and analyze. How can the organization of this data be improved to ensure ease of use, appropriate analysis, and equitable access?","title":"Problem Statement:"},{"location":"keysassignments/#references","text":"Fergason, R. L., Hunter, M. A., Laura, J. R., Hare, T. M., & U.S. Geological Survey. (2021). Analysis Ready Data Available Through the SpatioTemporal Asset Catalog (STAC) Specification: Investigating the Application to Planetary Data. 5 th Planetary Data and PSIDA 2021, 2549, 7023\u20137024. https://www.hou.usra.edu/meetings/planetdata2021/pdf/7023.pdf Kumar, L., & Mutanga, O. (2018). Google Earth Engine Applications Since Inception: Usage, Trends, and Potential. Remote Sensing, 10(10), 1509. https://doi.org/10.3390/rs10101509","title":"References"},{"location":"logbook/","text":"Logbook \u00b6 Day 1 (6/8) : Met with Dr. Swetnam and discussed project options and overall goals for the summer. Discussed working on the OpenDendro initiative or instead working with Google Earth Engine . Talked about how to setup this website using github: cloning Shruti's (a previous KEYS intern) repository and updating the information to match my own. Day 2 (6/9) : attempted to clone Shruti's repository through github's importer. However, this got stuck loading for 24+ hours so my progress was kind of at a standstill. Day 3 (6/10) : Cloning through github's importer never successfully loaded. I attempted to download all of Shruti's files to my computer and then add those files to my own repo to make a clone of her repo. However, Dr. Swetnam pointed out that after doing this I was still missing some key files and that the file paths and structure was incorrect. Day 4 (6/11) : Deleted contents of my new repository and this time cloned Shruti's repo through the terminal using git. After some trial and error (I had a very low level of familliarity with git) I got it to work! Day 5 (6/12) : Now that I had a successfully cloned repository, I worked on editing the mkdocs.yml to get my own website up and running. After setting up the basics and the URL, I added more personal information to further personalize my website. Day 6 (6/13) : Researched and enrolled in GitHub Education and CodeSpaces (turns out CodeSpaces isn't available as part of the student developer pack). Continued tweaking and cleaning up my GitHub site (read more of the mkdocs documentation to better understand this). Began research into Google Earth Engine and STAC Catalogs to build a foundation for my project this summer. Day 7-8 (6/14-15) : Gained increased familliarity with Google Earth Engine and accessed Google Earth Engine Developer. Forked Samapriya's awesome gee community datasets on GitHub to begin working with the repo with the intention of sending a pull request at the end of the summer to send it back to the main. Day 9 (6/16) : Continued to work with Google Earth Engine and began working with Jupyter Notebooks to better understand STAC catalogs. Worked to understand the formatting of the awesome-gee-community datasets by looking at the json files. Continued taking notes on both STAC catalogs and Google Earth Engine (adding these notes to the site tomorrow). Day 10 (6/17) : Continued to take notes to better understand the awesome-gee-community datasets and STAC catalogs. Began to transfer notes and information over into the github.io site (still need to make further progress with this). Experimented more with Jupyter Notebooks (need to commit more time to better understanding this). Talked to Dr. Swetnam about plans for meeting with Samapriya next week to discuss his vision for the awesome-gee-community datasets. Day 11 (6/20) : Worked through KEYS assignment on the introduction, background, and purpose of my research. Read additional research papers on both STAC catalogs and Google Earth Engine. Worked to create graphics in MKdocs to represent the structure of the awesome-gee-community datasets. Day 12 (6/21) : Today I fleshed out the STAC catalogs and GEE pages on my github.io site. Additionally, it turned out that my mkdocs diagrams are not rendering properly on my github.io site. I spent a long time researching the mkdocs and mermaid documentation for this, along with trying to debug it, but with no luck. I created a github issue about it explaining it more fully, but I hope to get this sorted out tomorrow. Experimented with visualizing different datasets on GEE and sifting through the GEE public data STAC catalog. Realized I should be taking more original pictures for my webpage and poster, so I will now start doing this as I move throughout the process. High on my personal agenda for tomorrow is to format all of the sources I've used/collected in APA style and properly cite them.","title":"Logbook"},{"location":"logbook/#logbook","text":"Day 1 (6/8) : Met with Dr. Swetnam and discussed project options and overall goals for the summer. Discussed working on the OpenDendro initiative or instead working with Google Earth Engine . Talked about how to setup this website using github: cloning Shruti's (a previous KEYS intern) repository and updating the information to match my own. Day 2 (6/9) : attempted to clone Shruti's repository through github's importer. However, this got stuck loading for 24+ hours so my progress was kind of at a standstill. Day 3 (6/10) : Cloning through github's importer never successfully loaded. I attempted to download all of Shruti's files to my computer and then add those files to my own repo to make a clone of her repo. However, Dr. Swetnam pointed out that after doing this I was still missing some key files and that the file paths and structure was incorrect. Day 4 (6/11) : Deleted contents of my new repository and this time cloned Shruti's repo through the terminal using git. After some trial and error (I had a very low level of familliarity with git) I got it to work! Day 5 (6/12) : Now that I had a successfully cloned repository, I worked on editing the mkdocs.yml to get my own website up and running. After setting up the basics and the URL, I added more personal information to further personalize my website. Day 6 (6/13) : Researched and enrolled in GitHub Education and CodeSpaces (turns out CodeSpaces isn't available as part of the student developer pack). Continued tweaking and cleaning up my GitHub site (read more of the mkdocs documentation to better understand this). Began research into Google Earth Engine and STAC Catalogs to build a foundation for my project this summer. Day 7-8 (6/14-15) : Gained increased familliarity with Google Earth Engine and accessed Google Earth Engine Developer. Forked Samapriya's awesome gee community datasets on GitHub to begin working with the repo with the intention of sending a pull request at the end of the summer to send it back to the main. Day 9 (6/16) : Continued to work with Google Earth Engine and began working with Jupyter Notebooks to better understand STAC catalogs. Worked to understand the formatting of the awesome-gee-community datasets by looking at the json files. Continued taking notes on both STAC catalogs and Google Earth Engine (adding these notes to the site tomorrow). Day 10 (6/17) : Continued to take notes to better understand the awesome-gee-community datasets and STAC catalogs. Began to transfer notes and information over into the github.io site (still need to make further progress with this). Experimented more with Jupyter Notebooks (need to commit more time to better understanding this). Talked to Dr. Swetnam about plans for meeting with Samapriya next week to discuss his vision for the awesome-gee-community datasets. Day 11 (6/20) : Worked through KEYS assignment on the introduction, background, and purpose of my research. Read additional research papers on both STAC catalogs and Google Earth Engine. Worked to create graphics in MKdocs to represent the structure of the awesome-gee-community datasets. Day 12 (6/21) : Today I fleshed out the STAC catalogs and GEE pages on my github.io site. Additionally, it turned out that my mkdocs diagrams are not rendering properly on my github.io site. I spent a long time researching the mkdocs and mermaid documentation for this, along with trying to debug it, but with no luck. I created a github issue about it explaining it more fully, but I hope to get this sorted out tomorrow. Experimented with visualizing different datasets on GEE and sifting through the GEE public data STAC catalog. Realized I should be taking more original pictures for my webpage and poster, so I will now start doing this as I move throughout the process. High on my personal agenda for tomorrow is to format all of the sources I've used/collected in APA style and properly cite them.","title":"Logbook"}]}